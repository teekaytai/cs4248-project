{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "J_WvG0K7rTtV"
      ],
      "toc_visible": true,
      "mount_file_id": "1gbe9GhIShbHhhSpDyYlgvcah5SbA0rWR",
      "authorship_tag": "ABX9TyPHdlSCqRKBM7GZc0FVR0xD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teekaytai/cs4248-project/blob/ahiyer/%5BWI_LOCNESS%5D_Final_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERAL"
      ],
      "metadata": {
        "id": "ktbtnzs2Z7wf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLwiD-WWzQJa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq_pDsxzyxUs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = '/content/drive/MyDrive/datasets/wi+locness/dataset_splits/test.json'\n",
        "train_path = '/content/drive/MyDrive/datasets/wi+locness/dataset_splits/train.json'\n",
        "val_path = '/content/drive/MyDrive/datasets/wi+locness/dataset_splits/val.json'"
      ],
      "metadata": {
        "id": "047V0fdl0DQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BdFn8rrV0Gzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize senteces with simple fuction\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ],
      "metadata": {
        "id": "RlYwxodA_j3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, Concatenate\n",
        "from tensorflow.keras import Input, Model"
      ],
      "metadata": {
        "id": "42UG93QD1jaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "n1sva9BVdZ6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "ylO5SNBy13HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "metadata": {
        "id": "7LBkg_gmpx9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import spacy"
      ],
      "metadata": {
        "id": "RvzyGVTlSSZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re"
      ],
      "metadata": {
        "id": "yu-rYda6sDtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRANSFORMERS"
      ],
      "metadata": {
        "id": "L6REUymO71qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORMERS (BART) : No Context\n",
        "\n",
        "> NO CONTEXT\n",
        "\n"
      ],
      "metadata": {
        "id": "ZErWSYtF0uge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Baseline\n",
        "Input: Error Sentences\n",
        "Output: Corrected Sentences\n",
        "'''\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import BartModel, BartForConditionalGeneration, BartTokenizerFast\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "\n",
        "# Configs\n",
        "BART_MODEL = 'gotutiyan/gec-bart-base'\n",
        "MAX_SOURCE_LENGTH = 240\n",
        "MAX_TARGET_LENGTH = 240\n",
        "NUM_EPOCHS = 10\n",
        "# Hugging face documentation reccomends 1e-4 or 3e-4 for T5\n",
        "LEARNING_RATE = 3e-4\n",
        "NUM_BEAMS = 5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BartTokenizerFast.from_pretrained(BART_MODEL)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "spacy_tokenizer = nlp.tokenizer\n",
        "\n",
        "def tokenize_source_sentences(sentences):\n",
        "    return tokenizer(\n",
        "        sentences,\n",
        "        padding = 'max_length',\n",
        "        max_length = MAX_SOURCE_LENGTH,\n",
        "        truncation = True,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "def tokenize_target_sentences(sentences):\n",
        "    tokenized = tokenizer(\n",
        "        sentences,\n",
        "        padding = 'max_length',\n",
        "        max_length = MAX_TARGET_LENGTH,\n",
        "        truncation = True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "    # Replace padding token ids of the labels by -100 so it's ignored by the loss\n",
        "    ids = tokenized.input_ids\n",
        "    ids[ids == tokenizer.pad_token_id] = -100\n",
        "    tokenized.input_ids = ids\n",
        "    return tokenized\n",
        "\n",
        "def preprocess_dataset(dataset, source_column_name, target_column_name):\n",
        "    tokenized_source = tokenize_source_sentences(dataset[source_column_name])\n",
        "    tokenized_target = tokenize_target_sentences(dataset[target_column_name])\n",
        "    input = {}\n",
        "    input['input_ids'] = tokenized_source['input_ids']\n",
        "    input['attention_mask'] = tokenized_source['attention_mask']\n",
        "    input['labels'] = tokenized_target['input_ids']\n",
        "    return input\n",
        "\n",
        "def train(train_dataset, eval_dataset, output_dir):\n",
        "    model = BartForConditionalGeneration.from_pretrained(BART_MODEL)\n",
        "    model.to(device)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir = output_dir,\n",
        "        num_train_epochs = NUM_EPOCHS,\n",
        "        evaluation_strategy = 'steps',\n",
        "        eval_steps = 500,\n",
        "        save_steps = 500,\n",
        "        learning_rate = LEARNING_RATE,\n",
        "        load_best_model_at_end = True,\n",
        "        save_total_limit = 2,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model = model,\n",
        "        args = training_args,\n",
        "        train_dataset = train_dataset,\n",
        "        eval_dataset = eval_dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "def generate(model_path, dataset):\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "\n",
        "    generated_sentences = []\n",
        "\n",
        "    for sample in dataset:\n",
        "        original = sample['original']\n",
        "        tokenized = tokenize_source_sentences([original])\n",
        "        generated = model.generate(\n",
        "            tokenized.input_ids,\n",
        "            max_length = MAX_TARGET_LENGTH,\n",
        "            num_beams = NUM_BEAMS,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        generated_sentence = tokenizer.decode(\n",
        "            generated[0],\n",
        "            skip_special_tokens=True,\n",
        "        )\n",
        "        # Retokenize sentence using spacy to restore correct spacing between tokens\n",
        "        # for accurate error correction score calculation\n",
        "        generated_sentence = ' '.join(tok.text for tok in spacy_tokenizer(generated_sentence))\n",
        "        generated_sentences.append(generated_sentence)\n",
        "\n",
        "    return generated_sentences\n",
        "\n",
        "def get_model(path):\n",
        "    model = BartForConditionalGeneration.from_pretrained(path)\n",
        "    model.to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "lNFMbTrM1FLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "GVSn07cb7h5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "id": "ZiwG-vrf7x9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_train = load_dataset('json', data_files=train_path, split='train')\n",
        "dataset_eval = load_dataset('json', data_files=val_path, split='train')\n",
        "\n",
        "preprocessed_train = dataset_train.map(\n",
        "    preprocess_dataset,\n",
        "    batched=True,\n",
        "    fn_kwargs={\"source_column_name\": \"original\", \"target_column_name\": \"corrected\"}\n",
        ")\n",
        "preprocessed_eval = dataset_eval.map(\n",
        "    preprocess_dataset,\n",
        "    batched=True,\n",
        "    fn_kwargs={\"source_column_name\": \"original\", \"target_column_name\": \"corrected\"}\n",
        ")\n",
        "\n",
        "train(preprocessed_train, preprocessed_eval, 'outputs/model_sentence_2')"
      ],
      "metadata": {
        "id": "6wlPoNS27Pho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install errant"
      ],
      "metadata": {
        "id": "DpfoJAqx1Udc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/outputs.zip /content/outputs"
      ],
      "metadata": {
        "id": "XvlY_VMp1mpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/outputs.zip\")"
      ],
      "metadata": {
        "id": "su6DkAVd1r3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import errant\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import namedtuple\n",
        "\n",
        "NOOP_EDIT = 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "annotator = errant.load('en', nlp)\n",
        "\n",
        "def generate_m2(input_sentences, output_sentences, output_path):\n",
        "    with open(output_path, 'w') as f:\n",
        "        for input, output in zip(input_sentences, output_sentences):\n",
        "            edits = annotator.annotate(annotator.parse(input), annotator.parse(output))\n",
        "            print('S', input, file=f)\n",
        "            if not edits:\n",
        "                print(NOOP_EDIT, file=f)\n",
        "            for edit in edits:\n",
        "                print(edit.to_m2(), file=f)\n",
        "            print(file=f)  # Blank divider line\n",
        "\n",
        "\n",
        "\n",
        "EDIT_OPS = {'M': 'Missing', 'U': 'Unnecessary', 'R': 'Replacement'}\n",
        "NOOP_EDIT_TYPE = 'noop'\n",
        "UNK_EDIT_TYPE = 'UNK'\n",
        "EDIT_TYPES = [\n",
        "    'ADJ', 'ADJ:FORM', 'ADV', 'CONJ', 'CONTR', 'DET', 'MORPH',\n",
        "    'NOUN', 'NOUN:INFL', 'NOUN:NUM', 'NOUN:POSS',\n",
        "    'ORTH', 'OTHER', 'PART', 'PREP', 'PRON', 'PUNCT', 'SPELL',\n",
        "    'VERB', 'VERB:FORM', 'VERB:INFL', 'VERB:SVA', 'VERB:TENSE', 'WO',\n",
        "]\n",
        "\n",
        "Edit = namedtuple('Edit', ['span', 'code', 'correction'])\n",
        "\n",
        "def load_edits(m2_file_path):\n",
        "    edits = []\n",
        "    with open(m2_file_path, 'r') as f:\n",
        "        for group in f.read().split('\\n\\n'):\n",
        "            if not group:\n",
        "                continue\n",
        "            sentence, *sent_edits = group.split('\\n')\n",
        "            edits.append([Edit(*e[2:].split('|||')[:3]) for e in sent_edits])\n",
        "    return edits\n",
        "\n",
        "def create_error_count_df(gold_edits, output_edits):\n",
        "    rows = [*EDIT_OPS.values(), *EDIT_TYPES, NOOP_EDIT_TYPE, UNK_EDIT_TYPE]\n",
        "    df = pd.DataFrame(0, index=rows, columns=['TP', 'FP', 'FN'])\n",
        "    for gold_sent_edits, output_sent_edits in zip(gold_edits, output_edits):\n",
        "        gold_set = set(gold_sent_edits)\n",
        "        out_set = set(output_sent_edits)\n",
        "        classified_edits = {\n",
        "            'TP': gold_set & out_set,\n",
        "            'FP': out_set - gold_set,\n",
        "            'FN': gold_set - out_set\n",
        "        }\n",
        "        for outcome, edits in classified_edits.items():\n",
        "            for edit in edits:\n",
        "                if edit.code in (NOOP_EDIT_TYPE, UNK_EDIT_TYPE):\n",
        "                    df.loc[edit.code, outcome] += 1\n",
        "                else:\n",
        "                    op, type_ = edit.code.split(':', maxsplit=1)\n",
        "                    df.loc[EDIT_OPS[op], outcome] += 1\n",
        "                    df.loc[type_, outcome] += 1\n",
        "    df['P'] = df['TP'] / (df['TP'] + df['FP'])\n",
        "    df['R'] = df['TP'] / (df['TP'] + df['FN'])\n",
        "    df['F0.5'] = (1 + 0.5**2) * ((df['P'] * df['R']) / (0.5**2 * df['P'] + df['R']))\n",
        "    return df\n",
        "\n",
        "def analyze_error_types(actual_path, predicted_path):\n",
        "    gold_edits = load_edits(actual_path)\n",
        "    output_edits = load_edits(predicted_path)\n",
        "    error_df = create_error_count_df(gold_edits, output_edits)\n",
        "    print(error_df)\n",
        "    sns.heatmap(error_df[['P', 'R', 'F0.5']], vmin=0.0, vmax=1.0, cmap='Reds', annot=True, yticklabels=True)\n",
        "    plt.show()\n",
        "\n",
        "def analyze_params(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"{total_params:,} total parameters.\")\n",
        "    total_trainable_params = sum(\n",
        "        p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"{total_trainable_params:,} training parameters.\")"
      ],
      "metadata": {
        "id": "e6ot7kGu1SG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'outputs/model_sentence_2/checkpoint-35000'\n",
        "\n",
        "analyze_params(get_model(model_path))\n",
        "\n",
        "dataset_test = load_dataset('json', data_files=test_path, split='train')\n",
        "generated_sentences = generate(model_path, dataset_test)\n",
        "\n",
        "generate_m2(dataset_test['original'], generated_sentences, model_path + '/gen.m2')\n",
        "\n",
        "analyze_error_types('/content/drive/MyDrive/datasets/wi+locness/dataset_splits/test.m2', model_path + '/gen.m2')"
      ],
      "metadata": {
        "id": "44AslVQsw_Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CUSTOM_TESTS = [\n",
        "    ['She saw a cat.', 'He screams out loud.'],  # PRON, VERB:TENSE\n",
        "    ['The P versus NP problem is an unsolved problem in computer science.', 'No one has solved them to this day.'],  # PRON\n",
        "    ['The Millennium Prize Problems are seven very complex mathematical problems.', 'No one has solved it to this day.'],\n",
        "    ['Car crashes are easily preventable.', 'Most cases occurred because the driver was careless.'],  # VERB:TENSE\n",
        "    ['A study was done on 1000 car crashes.', 'Most cases occur because the driver is careless.'],\n",
        "    [\"If he thinks about it more, I'm sure he'll figure something out.\", 'The right idea eventually came to him.'],  # VERB:TENSE\n",
        "    ['The right idea will eventually come to him.', 'Many weeks of effort finally paid off.'],\n",
        "    ['Everyone knows that cats are adorable.', 'But they make for great companions.'],  # CONJ\n",
        "    ['Cats can be annoying at times.', 'And they make for great companions.'],\n",
        "    ['I visit the apple store frequently.', \"I'm always eager to check out the latest phone.\"],  # ORTH\n",
        "    ['I visit the apple store frequently.', 'Fruit works great as a snack.'],\n",
        "    ['Tom told his sister there was a spider in her hair.', 'Cried out in alarm.'],  # PRON\n",
        "    ['There have been complaints about long queues in the canteens.', \"I'm looking them now.\"],  # PREP\n",
        "    [\"I lost my earphones earlier.\", \"I'm looking them now.\"]\n",
        "]\n",
        "\n",
        "def get_custom_tests():\n",
        "    dataset = []\n",
        "    for para in CUSTOM_TESTS:\n",
        "        for pos, sentence in enumerate(para):\n",
        "            dataset.append({\n",
        "                \"original\": sentence,\n",
        "                \"pos\": pos,\n",
        "                \"paragraph\": para\n",
        "            })\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "0HsD8uZj6mzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "custom_tests = get_custom_tests()\n",
        "print(custom_tests)\n",
        "dataset_test = Dataset.from_list(custom_tests)\n",
        "\n",
        "model_path = 'outputs/model_sentence_2/checkpoint-35000'\n",
        "generated_sentences = generate(model_path, dataset_test)\n",
        "\n",
        "with open(model_path + '/custom.txt', 'w') as f:\n",
        "    for line in generated_sentences:\n",
        "        f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "13MhkzB06jwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORMERS (BART) : Sentence Flank\n",
        "\n",
        "> SENTENCE FLANK\n",
        "\n"
      ],
      "metadata": {
        "id": "drVX7pbD81_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note that the parameters can be changed and finetuned as needed. For consistency's sake, we keep the same. Moreover, the 0/1, 1/0, 1/1 experiments are all done in this section by changing the relevant variables"
      ],
      "metadata": {
        "id": "pebTUB11R9b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Adding pre/post k sentences to the target sentence\n",
        "Input: Error sentence + k pre + j post\n",
        "Output: Corrected sentence\n",
        "'''\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import BartModel, BartForConditionalGeneration, BartTokenizerFast\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "\n",
        "# Configs\n",
        "BART_MODEL = 'gotutiyan/gec-bart-base'\n",
        "MAX_SOURCE_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 240\n",
        "NUM_EPOCHS = 7\n",
        "# Hugging face documentation reccomends 1e-4 or 3e-4 for T5\n",
        "LEARNING_RATE = 3e-4\n",
        "NUM_BEAMS = 5\n",
        "CONCAT_PARA_TOKEN = ' <cct> '\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BartTokenizerFast.from_pretrained(BART_MODEL)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "spacy_tokenizer = nlp.tokenizer\n",
        "\n",
        "def tokenize_source_sentences(sentences, paragraphs, sentence_positions, prec_range, post_range):\n",
        "    concatenated_sentences = []\n",
        "    for sentence, para, pos in zip(sentences, paragraphs, sentence_positions):\n",
        "        para_len = len(para)\n",
        "        concatenated = CONCAT_PARA_TOKEN.join(para[max(pos - prec_range, 0) : min(pos + post_range + 1, para_len)])\n",
        "        concatenated_sentences.append(concatenated)\n",
        "\n",
        "    return tokenizer(\n",
        "        sentences,\n",
        "        padding = 'max_length',\n",
        "        max_length = MAX_SOURCE_LENGTH,\n",
        "        truncation = True,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "def tokenize_target_sentences(sentences):\n",
        "    tokenized = tokenizer(\n",
        "        sentences,\n",
        "        padding = 'max_length',\n",
        "        max_length = MAX_TARGET_LENGTH,\n",
        "        truncation = True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "    # Replace padding token ids of the labels by -100 so it's ignored by the loss\n",
        "    ids = tokenized.input_ids\n",
        "    ids[ids == tokenizer.pad_token_id] = -100\n",
        "    tokenized.input_ids = ids\n",
        "    return tokenized\n",
        "\n",
        "def preprocess_dataset(dataset, source_column_name, target_column_name, para_column_name, pos_column_name, prec_range, post_range):\n",
        "    tokenized_source = tokenize_source_sentences(dataset[source_column_name], dataset[para_column_name], dataset[pos_column_name], prec_range, post_range)\n",
        "    tokenized_target = tokenize_target_sentences(dataset[target_column_name])\n",
        "    input = {}\n",
        "    input['input_ids'] = tokenized_source['input_ids']\n",
        "    input['attention_mask'] = tokenized_source['attention_mask']\n",
        "    input['labels'] = tokenized_target['input_ids']\n",
        "    return input\n",
        "\n",
        "def train(train_dataset, eval_dataset, output_dir):\n",
        "    model = BartForConditionalGeneration.from_pretrained(BART_MODEL)\n",
        "    model.to(device)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir = output_dir,\n",
        "        num_train_epochs = NUM_EPOCHS,\n",
        "        evaluation_strategy = 'steps',\n",
        "        eval_steps = 500,\n",
        "        save_steps = 500,\n",
        "        learning_rate = LEARNING_RATE,\n",
        "        load_best_model_at_end = True,\n",
        "        save_total_limit = 2,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model = model,\n",
        "        args = training_args,\n",
        "        train_dataset = train_dataset,\n",
        "        eval_dataset = eval_dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "def generate(model_path, dataset, prec_range, post_range):\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "\n",
        "    generated_sentences = []\n",
        "    i = 1\n",
        "\n",
        "    for sample in dataset:\n",
        "        print(i)\n",
        "        i+=1\n",
        "        original = sample['original']\n",
        "        para = sample['paragraph']\n",
        "        pos = sample['pos']\n",
        "        tokenized = tokenize_source_sentences([original], [para], [pos], prec_range, post_range)\n",
        "        generated = model.generate(\n",
        "            tokenized.input_ids,\n",
        "            max_length = MAX_TARGET_LENGTH,\n",
        "            num_beams = NUM_BEAMS,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        generated_sentence = tokenizer.decode(\n",
        "            generated[0],\n",
        "            skip_special_tokens=True,\n",
        "        )\n",
        "        # Retokenize sentence using spacy to restore correct spacing between tokens\n",
        "        # for accurate error correction score calculation\n",
        "        generated_sentence = ' '.join(tok.text for tok in spacy_tokenizer(generated_sentence))\n",
        "        generated_sentences.append(generated_sentence)\n",
        "\n",
        "    return generated_sentences\n",
        "\n",
        "def get_model(path):\n",
        "    model = BartForConditionalGeneration.from_pretrained(path)\n",
        "    model.to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xF5W9d9V9WlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "id": "2JbIyMACaqUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_train = load_dataset('json', data_files=train_path, split='train')\n",
        "dataset_eval = load_dataset('json', data_files=val_path, split='train')\n",
        "\n",
        "dataset_kwargs = {\n",
        "        \"source_column_name\": \"original\",\n",
        "        \"target_column_name\": \"corrected\",\n",
        "        \"para_column_name\": \"paragraph\",\n",
        "        \"pos_column_name\": \"pos\",\n",
        "        \"prec_range\": 1,\n",
        "        \"post_range\": 0,\n",
        "        }\n",
        "\n",
        "preprocessed_train = dataset_train.map(\n",
        "    preprocess_dataset,\n",
        "    batched=True,\n",
        "    fn_kwargs=dataset_kwargs\n",
        ")\n",
        "preprocessed_eval = dataset_eval.map(\n",
        "    preprocess_dataset,\n",
        "    batched=True,\n",
        "    fn_kwargs=dataset_kwargs\n",
        ")\n",
        "\n",
        "train(preprocessed_train, preprocessed_eval, 'outputs_sf/model_sentence_append_source/1_1')"
      ],
      "metadata": {
        "id": "wFurGSYw-yqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/outputs_sf.zip /content/outputs_sf"
      ],
      "metadata": {
        "id": "jWThQsUa81_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/outputs_sf.zip\")"
      ],
      "metadata": {
        "id": "2b99Gror81_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install errant"
      ],
      "metadata": {
        "id": "CsOCOLF0Giln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import errant\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import namedtuple\n",
        "\n",
        "NOOP_EDIT = 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "annotator = errant.load('en', nlp)\n",
        "\n",
        "def generate_m2(input_sentences, output_sentences, output_path):\n",
        "    with open(output_path, 'w') as f:\n",
        "        for input, output in zip(input_sentences, output_sentences):\n",
        "            edits = annotator.annotate(annotator.parse(input), annotator.parse(output))\n",
        "            print('S', input, file=f)\n",
        "            if not edits:\n",
        "                print(NOOP_EDIT, file=f)\n",
        "            for edit in edits:\n",
        "                print(edit.to_m2(), file=f)\n",
        "            print(file=f)  # Blank divider line\n",
        "\n",
        "\n",
        "\n",
        "EDIT_OPS = {'M': 'Missing', 'U': 'Unnecessary', 'R': 'Replacement'}\n",
        "NOOP_EDIT_TYPE = 'noop'\n",
        "UNK_EDIT_TYPE = 'UNK'\n",
        "EDIT_TYPES = [\n",
        "    'ADJ', 'ADJ:FORM', 'ADV', 'CONJ', 'CONTR', 'DET', 'MORPH',\n",
        "    'NOUN', 'NOUN:INFL', 'NOUN:NUM', 'NOUN:POSS',\n",
        "    'ORTH', 'OTHER', 'PART', 'PREP', 'PRON', 'PUNCT', 'SPELL',\n",
        "    'VERB', 'VERB:FORM', 'VERB:INFL', 'VERB:SVA', 'VERB:TENSE', 'WO',\n",
        "]\n",
        "\n",
        "Edit = namedtuple('Edit', ['span', 'code', 'correction'])\n",
        "\n",
        "def load_edits(m2_file_path):\n",
        "    edits = []\n",
        "    with open(m2_file_path, 'r') as f:\n",
        "        for group in f.read().split('\\n\\n'):\n",
        "            if not group:\n",
        "                continue\n",
        "            sentence, *sent_edits = group.split('\\n')\n",
        "            edits.append([Edit(*e[2:].split('|||')[:3]) for e in sent_edits])\n",
        "    return edits\n",
        "\n",
        "def create_error_count_df(gold_edits, output_edits):\n",
        "    rows = [*EDIT_OPS.values(), *EDIT_TYPES, NOOP_EDIT_TYPE, UNK_EDIT_TYPE]\n",
        "    df = pd.DataFrame(0, index=rows, columns=['TP', 'FP', 'FN'])\n",
        "    for gold_sent_edits, output_sent_edits in zip(gold_edits, output_edits):\n",
        "        gold_set = set(gold_sent_edits)\n",
        "        out_set = set(output_sent_edits)\n",
        "        classified_edits = {\n",
        "            'TP': gold_set & out_set,\n",
        "            'FP': out_set - gold_set,\n",
        "            'FN': gold_set - out_set\n",
        "        }\n",
        "        for outcome, edits in classified_edits.items():\n",
        "            for edit in edits:\n",
        "                if edit.code in (NOOP_EDIT_TYPE, UNK_EDIT_TYPE):\n",
        "                    df.loc[edit.code, outcome] += 1\n",
        "                else:\n",
        "                    op, type_ = edit.code.split(':', maxsplit=1)\n",
        "                    df.loc[EDIT_OPS[op], outcome] += 1\n",
        "                    df.loc[type_, outcome] += 1\n",
        "    df['P'] = df['TP'] / (df['TP'] + df['FP'])\n",
        "    df['R'] = df['TP'] / (df['TP'] + df['FN'])\n",
        "    df['F0.5'] = (1 + 0.5**2) * ((df['P'] * df['R']) / (0.5**2 * df['P'] + df['R']))\n",
        "    return df\n",
        "\n",
        "def analyze_error_types(actual_path, predicted_path):\n",
        "    gold_edits = load_edits(actual_path)\n",
        "    output_edits = load_edits(predicted_path)\n",
        "    error_df = create_error_count_df(gold_edits, output_edits)\n",
        "    print(error_df)\n",
        "    sns.heatmap(error_df[['P', 'R', 'F0.5']], vmin=0.0, vmax=1.0, cmap='Reds', annot=True, yticklabels=True)\n",
        "    plt.show()\n",
        "\n",
        "def analyze_params(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"{total_params:,} total parameters.\")\n",
        "    total_trainable_params = sum(\n",
        "        p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"{total_trainable_params:,} training parameters.\")"
      ],
      "metadata": {
        "id": "mQZ6mhX081_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = load_dataset('json', data_files=test_path, split='train')\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "k5c2OE2NsBjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "model_path = 'outputs_sf/model_sentence_append_source/1_1/checkpoint-7000'\n",
        "\n",
        "analyze_params(get_model(model_path))\n",
        "\n",
        "dataset_test = load_dataset('json', data_files=test_path, split='train')\n",
        "generated_sentences = generate(\n",
        "    model_path,\n",
        "    dataset_test,\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "\n",
        "generate_m2(dataset_test['original'], generated_sentences, model_path + '/gen.m2')\n",
        "\n",
        "analyze_error_types('/content/drive/MyDrive/datasets/wi+locness/dataset_splits/test.m2', model_path + '/gen.m2')"
      ],
      "metadata": {
        "id": "BvYHRs7rCP9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_error_types('/content/drive/MyDrive/datasets/wi+locness/dataset_splits/test.m2', model_path + '/gen.m2')"
      ],
      "metadata": {
        "id": "qjbm32Ho33PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!errant_compare -hyp \"/content/outputs_sf/model_sentence_append_source/1_1/checkpoint-7000/gen.m2\" -ref  '/content/drive/MyDrive/datasets/wi+locness/dataset_splits/test.m2'"
      ],
      "metadata": {
        "id": "4nsMVWn0KNIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!errant_compare -hyp \"/content/bart_simple_gen.m2\" -ref  '/content/drive/MyDrive/datasets/wi+locness/dataset_splits/test.m2'"
      ],
      "metadata": {
        "id": "JdVuoaEUJMo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!errant_compare -hyp \"/content/bart_sf8500_10_gen.m2\" -ref  '/content/drive/MyDrive/datasets/wi+locness/dataset_splits/test.m2'"
      ],
      "metadata": {
        "id": "ul6dBUkPRJlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CUSTOM_TESTS = [\n",
        "    ['She saw a cat.', 'He screams out loud.'],  # PRON, VERB:TENSE\n",
        "    ['The P versus NP problem is an unsolved problem in computer science.', 'No one has solved them to this day.'],  # PRON\n",
        "    ['The Millennium Prize Problems are seven very complex mathematical problems.', 'No one has solved it to this day.'],\n",
        "    ['Car crashes are easily preventable.', 'Most cases occurred because the driver was careless.'],  # VERB:TENSE\n",
        "    ['A study was done on 1000 car crashes.', 'Most cases occur because the driver is careless.'],\n",
        "    [\"If he thinks about it more, I'm sure he'll figure something out.\", 'The right idea eventually came to him.'],  # VERB:TENSE\n",
        "    ['The right idea will eventually come to him.', 'Many weeks of effort finally paid off.'],\n",
        "    ['Everyone knows that cats are adorable.', 'But they make for great companions.'],  # CONJ\n",
        "    ['Cats can be annoying at times.', 'And they make for great companions.'],\n",
        "    ['I visit the apple store frequently.', \"I'm always eager to check out the latest phone.\"],  # ORTH\n",
        "    ['I visit the apple store frequently.', 'Fruit works great as a snack.'],\n",
        "    ['Tom told his sister there was a spider in her hair.', 'Cried out in alarm.'],  # PRON\n",
        "    ['There have been complaints about long queues in the canteens.', \"I'm looking them now.\"],  # PREP\n",
        "    [\"I lost my earphones earlier.\", \"I'm looking them now.\"]\n",
        "]\n",
        "\n",
        "def get_custom_tests():\n",
        "    dataset = []\n",
        "    for para in CUSTOM_TESTS:\n",
        "        for pos, sentence in enumerate(para):\n",
        "            dataset.append({\n",
        "                \"original\": sentence,\n",
        "                \"pos\": pos,\n",
        "                \"paragraph\": para\n",
        "            })\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "eSR7Fc9881_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "custom_tests = get_custom_tests()\n",
        "dataset_test = Dataset.from_list(custom_tests)\n",
        "\n",
        "model_path = 'outputs_sf/model_sentence_append_source/1_1/checkpoint-7000'\n",
        "generated_sentences = generate(model_path, dataset_test, 1, 0)\n",
        "\n",
        "with open(model_path + '/custom.txt', 'w') as f:\n",
        "    for line in generated_sentences:\n",
        "        f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "7hMctQKiwTYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/outputs_sf/model_sentence_append_source/1_1/checkpoint-7000/gen.m2')"
      ],
      "metadata": {
        "id": "tlromZ3qLips"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('/content/outputs_sf/model_sentence_append_source/1_1/checkpoint-7000/custom.txt')"
      ],
      "metadata": {
        "id": "In4sDdsrLl-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENTS"
      ],
      "metadata": {
        "id": "15RLNM0kSTTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORMER PERFORMANCE RAW"
      ],
      "metadata": {
        "id": "2GivaJCp5HsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Adding pre/post k sentences to the target sentence\n",
        "Input: Error sentence + k pre + j post\n",
        "Output: Corrected sentence\n",
        "'''\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import BartModel, BartForConditionalGeneration, BartTokenizerFast\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "\n",
        "# Configs\n",
        "BART_MODEL = 'gotutiyan/gec-bart-base'\n",
        "MAX_SOURCE_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 240\n",
        "NUM_EPOCHS = 7\n",
        "# Hugging face documentation reccomends 1e-4 or 3e-4 for T5\n",
        "LEARNING_RATE = 3e-4\n",
        "NUM_BEAMS = 5\n",
        "CONCAT_PARA_TOKEN = ' <cct> '\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BartTokenizerFast.from_pretrained(BART_MODEL)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "spacy_tokenizer = nlp.tokenizer\n",
        "\n",
        "def tokenize_target_sentences(sentences):\n",
        "    tokenized = tokenizer(\n",
        "        sentences,\n",
        "        padding = 'max_length',\n",
        "        max_length = MAX_TARGET_LENGTH,\n",
        "        truncation = True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "    # Replace padding token ids of the labels by -100 so it's ignored by the loss\n",
        "    ids = tokenized.input_ids\n",
        "    ids[ids == tokenizer.pad_token_id] = -100\n",
        "    tokenized.input_ids = ids\n",
        "    return tokenized\n",
        "\n",
        "def preprocess_dataset(dataset, source_column_name, target_column_name, para_column_name, pos_column_name, prec_range, post_range):\n",
        "    tokenized_source = tokenize_source_sentences(dataset[source_column_name], dataset[para_column_name], dataset[pos_column_name], prec_range, post_range)\n",
        "    tokenized_target = tokenize_target_sentences(dataset[target_column_name])\n",
        "    input = {}\n",
        "    input['input_ids'] = tokenized_source['input_ids']\n",
        "    input['attention_mask'] = tokenized_source['attention_mask']\n",
        "    input['labels'] = tokenized_target['input_ids']\n",
        "    return input\n",
        "\n",
        "def generate(model_path, dataset, prec_range, post_range):\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "\n",
        "    generated_sentences = []\n",
        "    i = 1\n",
        "\n",
        "    for sample in dataset:\n",
        "        print(i)\n",
        "        i+=1\n",
        "        original = sample['original']\n",
        "        para = sample['paragraph']\n",
        "        pos = sample['pos']\n",
        "        tokenized = tokenize_source_sentences([original], [para], [pos], prec_range, post_range)\n",
        "        generated = model.generate(\n",
        "            tokenized.input_ids,\n",
        "            max_length = MAX_TARGET_LENGTH,\n",
        "            num_beams = NUM_BEAMS,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        generated_sentence = tokenizer.decode(\n",
        "            generated[0],\n",
        "            skip_special_tokens=True,\n",
        "        )\n",
        "        # Retokenize sentence using spacy to restore correct spacing between tokens\n",
        "        # for accurate error correction score calculation\n",
        "        generated_sentence = ' '.join(tok.text for tok in spacy_tokenizer(generated_sentence))\n",
        "        generated_sentences.append(generated_sentence)\n",
        "\n",
        "    return generated_sentences\n",
        "\n",
        "def get_model(path):\n",
        "    model = BartForConditionalGeneration.from_pretrained(path)\n",
        "    model.to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "5jYsgkNz5LVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "4fVWuJZL6F7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "ZZxSIU6u6NL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text-generation pipeline for text correction\n",
        "corrector = pipeline(\"text2text-generation\", \"pszemraj/bart-base-grammar-synthesis\")"
      ],
      "metadata": {
        "id": "sGFItoE16zG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = load_dataset('json', data_files=test_path, split='train')\n",
        "def generated_sentences():\n",
        "  i = 0\n",
        "  for text in dataset_test:\n",
        "    if i > 3:\n",
        "      break\n",
        "    else:\n",
        "      print(corrector(text[\"original\"])[0][\"generated_text\"])\n",
        "      i+=1"
      ],
      "metadata": {
        "id": "nzzV5OmL5tJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sentences()"
      ],
      "metadata": {
        "id": "0m9Lg40_5-Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sentences_raw = np.array([])"
      ],
      "metadata": {
        "id": "zW8mmQvU6hy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "for text in dataset_test:\n",
        "  if i < 3:\n",
        "    print(generated_sentences_raw)\n",
        "    generated_sentences_raw = np.append(generated_sentences_raw, corrector(text[\"original\"])[0][\"generated_text\"])\n",
        "    i += 1\n",
        "  else:\n",
        "    print(i)\n",
        "    i += 1\n",
        "    org = text[\"original\"]\n",
        "    generated_sentences_raw = np.append(generated_sentences_raw, corrector(text[\"original\"])[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "ARmTyc1U6fbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "model_path = 'pszemraj/bart-base-grammar-synthesis'\n",
        "\n",
        "analyze_params(get_model(model_path))\n",
        "\n",
        "dataset_test = load_dataset('json', data_files=test_path, split='train')\n",
        "def generated_sentences():\n",
        "  for text in dataset_text:\n",
        "    print(text)\n",
        "\n",
        "generate_m2(dataset_test['original'], generated_sentences, '/gen.m2')\n",
        "\n",
        "analyze_error_types('/content/drive/MyDrive/datasets/wi+locness/dataset_splits/test.m2', '/gen.m2')"
      ],
      "metadata": {
        "id": "pjuVs4zm5aeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiLSTM Grammar Correction Experiements (Archived)\n",
        "\n",
        "Credit:\n",
        "\n",
        "https://medium.com/geekculture/neural-machine-translation-using-seq2seq-model-with-attention-9faea357d70b\n",
        "\n",
        "https://shreelakshmigp1995.medium.com/grammatical-error-correction-using-deep-learning-c36824de184\n",
        "\n",
        "https://colab.research.google.com/drive/1XrjPL3O_szhahYZW0z9yhCl9qvIcJJYW\n",
        "\n",
        "https://suraj1997lodh.medium.com/grammar-error-handling-and-correction-with-dataset-creation-e446fa6863b8\n",
        "\n",
        "https://medium.com/analytics-vidhya/grammatical-error-correction-using-neural-networks-aaf3e9fc91c"
      ],
      "metadata": {
        "id": "J_WvG0K7rTtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BILSTM NO CONTEXT\n"
      ],
      "metadata": {
        "id": "PXAFcS3FZ-nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_json(train_path)\n",
        "val_df = pd.read_json(val_path)\n",
        "test_df = pd.read_json(test_path)"
      ],
      "metadata": {
        "id": "zhq0HVC60IPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_train_val_df = pd.concat([train_df, val_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "yl-A7L0xvnV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding special tokens\n",
        "combined_train_val_df['corrected'] =combined_train_val_df.corrected.apply(lambda x: 'sos '+ x + ' eos')\n",
        "\n",
        "# Convert into list of sentence we need list to pass in tokenizer\n",
        "org_texts = combined_train_val_df.original.to_list()\n",
        "cor_texts = combined_train_val_df.corrected.to_list()"
      ],
      "metadata": {
        "id": "tHOJI2BNdhup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sent(text):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(text)\n",
        "\n",
        "  return tokenizer, tokenizer.texts_to_sequences(text)"
      ],
      "metadata": {
        "id": "E1925fWIdggG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "org_tokenizer, org_encoded= tokenize_sent(text= org_texts)\n",
        "cor_tokenizer, cor_encoded= tokenize_sent(text= cor_texts)\n",
        "\n",
        "# Original Word --> index dictionary\n",
        "org_index_word = org_tokenizer.index_word\n",
        "\n",
        "# Orignal Index --> word dictionary\n",
        "org_word_index= org_tokenizer.word_index\n",
        "\n",
        "# size of Original vocabulary for encoder input\n",
        "# For zero padding we have to add +1 in size\n",
        "ORG_VOCAB_SIZE = len(org_tokenizer.word_counts)+1\n",
        "\n",
        "# Corrected Word --> index dict\n",
        "cor_word_index= cor_tokenizer.word_index\n",
        "\n",
        "# Corrected Index --> word dict\n",
        "cor_index_word = cor_tokenizer.index_word\n",
        "\n",
        "# Cor vocab size for decoder output\n",
        "COR_VOCAB_SIZE=len(cor_tokenizer.word_counts)+1"
      ],
      "metadata": {
        "id": "S9YCFpatb5Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting max length of org and cor sentences\n",
        "max_org_len = 0\n",
        "for i in range(len(org_encoded)):\n",
        "  if len(org_encoded[i]) > max_org_len:\n",
        "    max_org_len= len(org_encoded[i])\n",
        "\n",
        "max_cor_len = 0\n",
        "for i in range(len(cor_encoded)):\n",
        "  if len(org_encoded[i]) > max_cor_len:\n",
        "    max_cor_len= len(cor_encoded[i])\n"
      ],
      "metadata": {
        "id": "KCGrcf0Mdptl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding both\n",
        "org_padded = pad_sequences(org_encoded, maxlen=max_org_len, padding='post')\n",
        "cor_padded = pad_sequences(cor_encoded, maxlen=max_cor_len, padding='post')\n",
        "\n",
        "# Convert to array\n",
        "org_padded= np.array(org_padded)\n",
        "cor_padded= np.array(cor_padded)"
      ],
      "metadata": {
        "id": "qn4jOK0cdsX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val = org_padded[:28066+1], org_padded[28066+1:]\n",
        "y_train, y_val = cor_padded[:28066+1], cor_padded[28066+1:]"
      ],
      "metadata": {
        "id": "kDtr9AvPxHGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder input\n",
        "encoder_inputs = Input(shape=(max_org_len,))\n",
        "\n",
        "# Embedding layer- i am using 1024 output-dim for embedding you can try diff values 100,256,512,1000\n",
        "enc_emb = Embedding(ORG_VOCAB_SIZE, 1024)(encoder_inputs)\n",
        "\n",
        "# Bidirectional lstm layer\n",
        "enc_lstm1 = Bidirectional(LSTM(256,return_sequences=True,return_state=True))\n",
        "encoder_outputs1, forw_state_h, forw_state_c, back_state_h, back_state_c = enc_lstm1(enc_emb)\n",
        "\n",
        "# Concatenate both h and c\n",
        "final_enc_h = Concatenate()([forw_state_h,back_state_h])\n",
        "final_enc_c = Concatenate()([forw_state_c,back_state_c])\n",
        "\n",
        "# get Context vector\n",
        "encoder_states =[final_enc_h, final_enc_c]"
      ],
      "metadata": {
        "id": "ue7KaBltZ-Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"attention.ipynb\n",
        "Automatically generated by Colaboratory.\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1XrjPL3O_szhahYZW0z9yhCl9qvIcJJYW\n",
        "\"\"\"\n",
        "\n",
        "class BahdAttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BahdAttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(BahdAttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "metadata": {
        "id": "CUlyfHUpA4cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model"
      ],
      "metadata": {
        "id": "zxqk2FkitpL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# decoder embedding with same number as encoder embedding\n",
        "dec_emb_layer = Embedding(COR_VOCAB_SIZE, 1024)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)   # apply this way because we need embedding layer for prediction\n",
        "\n",
        "# In encoder we used Bidirectional so it's having two LSTM's so we have to take double units(256*2=512) for single decoder lstm\n",
        "# LSTM using encoder's final states as initial state\n",
        "decoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "# Using Attention Layer\n",
        "attention_layer = BahdAttentionLayer()\n",
        "attention_result, attention_weights = attention_layer([encoder_outputs1, decoder_outputs])\n",
        "\n",
        "# Concat attention output and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])\n",
        "\n",
        "# Dense layer with softmax\n",
        "decoder_dense = Dense(COR_VOCAB_SIZE, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "Z5HzkS2QsV02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint = ModelCheckpoint(\"MODEL_CHECKPOINTS\")\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "callbacks_list = [checkpoint, early_stopping]\n",
        "\n",
        "# Training set\n",
        "encoder_input_data = X_train\n",
        "# To make same as target data skip last number which is just padding\n",
        "decoder_input_data = y_train[:,:-1]\n",
        "# Decoder target data has to be one step ahead so we are taking from 1 as told in keras docs\n",
        "decoder_target_data =  y_train[:,1:]\n",
        "\n",
        "# devlopment set\n",
        "encoder_input_test = X_val\n",
        "decoder_input_test = y_val[:,:-1]\n",
        "decoder_target_test=  y_val[:,1:]"
      ],
      "metadata": {
        "id": "EvtD4YnRuEBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([encoder_input_data, decoder_input_data],decoder_target_data,\n",
        "                    epochs=5,\n",
        "                    batch_size=32,\n",
        "                    validation_data = ([encoder_input_test, decoder_input_test],decoder_target_test),\n",
        "                    callbacks= callbacks_list)"
      ],
      "metadata": {
        "id": "wGohy57PtUWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"model.h5\") # can give whole path to save model"
      ],
      "metadata": {
        "id": "_vqYQD1F6UH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/outputs.zip /content/outputs"
      ],
      "metadata": {
        "id": "y0d4RGuWGkEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Its good to restart runtime and create model and load weights\n",
        "model.load_weights(\"model.h5\")\n",
        "\n",
        "# INFERENCE MODEL\n",
        "# encoder Inference model\n",
        "encoder_model = Model(encoder_inputs, outputs = [encoder_outputs1, final_enc_h, final_enc_c])\n",
        "\n",
        "# Decoder Inference\n",
        "decoder_state_h = Input(shape=(512,)) # This numbers has to be same as units of lstm's on which model is trained\n",
        "decoder_state_c = Input(shape=(512,))\n",
        "\n",
        "# we need hidden state for attention layer\n",
        "# 36 is maximum length if english sentence It has to same as input taken by attention layer can see in model plot\n",
        "decoder_hidden_state_input = Input(shape=(36,512))\n",
        "# get decoder states\n",
        "dec_states = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "# embedding layer\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states)\n",
        "\n",
        "# Attention inference\n",
        "attention_result_inf, attention_weights_inf = attention_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([decoder_outputs2, attention_result_inf])\n",
        "\n",
        "dec_states2= [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_concat_input_inf)\n",
        "\n",
        "# get decoder model\n",
        "decoder_model= Model(\n",
        "                    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_h, decoder_state_c],\n",
        "                     [decoder_outputs2]+ dec_states2)"
      ],
      "metadata": {
        "id": "Cs1suZ7aUJ49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicted_sentence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    enc_output, enc_h, enc_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = cor_word_index['sos']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [enc_output, enc_h, enc_c ])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        # convert max index number to marathi word\n",
        "        sampled_char = cor_index_word[sampled_token_index]\n",
        "        # aapend it to decoded sent\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length or find stop token.\n",
        "        if (sampled_char == 'eos' or len(decoded_sentence.split()) >= max_org_len):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        enc_h, enc_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "sMwbr9jJUrKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cor_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0 :\n",
        "        sentence =sentence +cor_index_word[i]+' '\n",
        "    return sentence\n",
        "\n",
        "def get_org_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0:\n",
        "        sentence =sentence +org_index_word[i]+' '\n",
        "    return sentence\n",
        "\n",
        "# # using simple loop we will take 15 random numbers from x_test and get results\n",
        "# for i in np.random.randint(10, 1000, size=15):\n",
        "#   print(\"Org Sentence:\",get_org_sentence(X_test[i]))\n",
        "#   print(\"Cor Sentence:\",get_cor_sentence(y_test[i])[4:-4])\n",
        "#   # Before passing input it has to be reshape as following\n",
        "#   print(\"Predicted Cor:\",get_predicted_sentence(X_test[i].reshape(1,max_org_len))[:-4])\n"
      ],
      "metadata": {
        "id": "DBBqAmSqVAVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install errant"
      ],
      "metadata": {
        "id": "7wvLQLHmTpwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import errant\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import namedtuple\n",
        "\n",
        "NOOP_EDIT = 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "annotator = errant.load('en', nlp)\n",
        "\n",
        "def generate_m2(input_sentences, output_sentences, output_path):\n",
        "    with open(output_path, 'w') as f:\n",
        "        for input, output in zip(input_sentences, output_sentences):\n",
        "            edits = annotator.annotate(annotator.parse(input), annotator.parse(output))\n",
        "            print('S', input, file=f)\n",
        "            if not edits:\n",
        "                print(NOOP_EDIT, file=f)\n",
        "            for edit in edits:\n",
        "                print(edit.to_m2(), file=f)\n",
        "            print(file=f)  # Blank divider line\n",
        "\n",
        "\n",
        "\n",
        "EDIT_OPS = {'M': 'Missing', 'U': 'Unnecessary', 'R': 'Replacement'}\n",
        "NOOP_EDIT_TYPE = 'noop'\n",
        "UNK_EDIT_TYPE = 'UNK'\n",
        "EDIT_TYPES = [\n",
        "    'ADJ', 'ADJ:FORM', 'ADV', 'CONJ', 'CONTR', 'DET', 'MORPH',\n",
        "    'NOUN', 'NOUN:INFL', 'NOUN:NUM', 'NOUN:POSS',\n",
        "    'ORTH', 'OTHER', 'PART', 'PREP', 'PRON', 'PUNCT', 'SPELL',\n",
        "    'VERB', 'VERB:FORM', 'VERB:INFL', 'VERB:SVA', 'VERB:TENSE', 'WO',\n",
        "]\n",
        "\n",
        "Edit = namedtuple('Edit', ['span', 'code', 'correction'])\n",
        "\n",
        "def load_edits(m2_file_path):\n",
        "    edits = []\n",
        "    with open(m2_file_path, 'r') as f:\n",
        "        for group in f.read().split('\\n\\n'):\n",
        "            if not group:\n",
        "                continue\n",
        "            sentence, *sent_edits = group.split('\\n')\n",
        "            edits.append([Edit(*e[2:].split('|||')[:3]) for e in sent_edits])\n",
        "    return edits\n",
        "\n",
        "def create_error_count_df(gold_edits, output_edits):\n",
        "    rows = [*EDIT_OPS.values(), *EDIT_TYPES, NOOP_EDIT_TYPE, UNK_EDIT_TYPE]\n",
        "    df = pd.DataFrame(0, index=rows, columns=['TP', 'FP', 'FN'])\n",
        "    for gold_sent_edits, output_sent_edits in zip(gold_edits, output_edits):\n",
        "        gold_set = set(gold_sent_edits)\n",
        "        out_set = set(output_sent_edits)\n",
        "        classified_edits = {\n",
        "            'TP': gold_set & out_set,\n",
        "            'FP': out_set - gold_set,\n",
        "            'FN': gold_set - out_set\n",
        "        }\n",
        "        for outcome, edits in classified_edits.items():\n",
        "            for edit in edits:\n",
        "                if edit.code in (NOOP_EDIT_TYPE, UNK_EDIT_TYPE):\n",
        "                    df.loc[edit.code, outcome] += 1\n",
        "                else:\n",
        "                    op, type_ = edit.code.split(':', maxsplit=1)\n",
        "                    df.loc[EDIT_OPS[op], outcome] += 1\n",
        "                    df.loc[type_, outcome] += 1\n",
        "    df['P'] = df['TP'] / (df['TP'] + df['FP'])\n",
        "    df['R'] = df['TP'] / (df['TP'] + df['FN'])\n",
        "    df['F0.5'] = (1 + 0.5**2) * ((df['P'] * df['R']) / (0.5**2 * df['P'] + df['R']))\n",
        "    return df\n",
        "\n",
        "def analyze_error_types(actual_path, predicted_path):\n",
        "    gold_edits = load_edits(actual_path)\n",
        "    output_edits = load_edits(predicted_path)\n",
        "    error_df = create_error_count_df(gold_edits, output_edits)\n",
        "    print(error_df)\n",
        "    sns.heatmap(error_df[['P', 'R', 'F0.5']], vmin=0.0, vmax=1.0, cmap='Reds', annot=True, yticklabels=True)\n",
        "    plt.show()\n",
        "\n",
        "def analyze_params(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"{total_params:,} total parameters.\")\n",
        "    total_trainable_params = sum(\n",
        "        p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"{total_trainable_params:,} training parameters.\")"
      ],
      "metadata": {
        "id": "0xKHZslD7MQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xt_2 = np.arange( X_val.shape[ 0 ] )\n",
        "np.random.shuffle(xt_2)"
      ],
      "metadata": {
        "id": "E0OPvnDBW_vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xt_2"
      ],
      "metadata": {
        "id": "qVgPmWWQXoYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slice1 = X_val[ xt_2[ :100 ], : ]"
      ],
      "metadata": {
        "id": "eWF0vmL4X2Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cor_test_cases():\n",
        "  generated = np.array([])\n",
        "  for i in range(slice1.shape[0]):\n",
        "    predicted = get_predicted_sentence(slice1[i].reshape(1,max_org_len))[:-4]\n",
        "    print(i)\n",
        "    generated = np.append(generated, predicted)\n",
        "  return generated\n"
      ],
      "metadata": {
        "id": "E82YcUJ68ekg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sentences = get_cor_test_cases()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VWj4gYu889BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = map(get_org_sentence, slice1)"
      ],
      "metadata": {
        "id": "apPjMD8fZY1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(x)"
      ],
      "metadata": {
        "id": "w1JjAOzAZtMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sentences"
      ],
      "metadata": {
        "id": "VOPOMolBZ82P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BILSTM (SENTENCE PAD LEFT + RIGHT)"
      ],
      "metadata": {
        "id": "HgodCyZDkfd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "train_df = pd.read_json(train_path)\n",
        "val_df = pd.read_json(val_path)\n",
        "test_df = pd.read_json(test_path)\n",
        "\n",
        "def make_context_sents(para, pos):\n",
        "    if pos + 1 < len(para) and pos - 1 >= 0:\n",
        "        return para[pos-1: pos+2]\n",
        "    elif pos - 1 >= 0:\n",
        "        return para[pos - 1: pos + 1]\n",
        "    elif pos + 1 < len(para):\n",
        "        return para[pos: pos + 2]\n",
        "    else:\n",
        "        return para[pos]\n",
        "\n",
        "\n",
        "train_df['original_with_context'] = train_df.apply(lambda x: make_context_sents(x.paragraph, x.pos), axis = 1)\n",
        "train_df['original_with_context'] = train_df['original_with_context'].apply(lambda x: (' '.join(x)).strip())\n",
        "\n",
        "test_df['original_with_context'] = test_df.apply(lambda x: make_context_sents(x.paragraph, x.pos), axis = 1)\n",
        "test_df['original_with_context'] = test_df['original_with_context'].apply(lambda x: (' '.join(x)).strip())\n",
        "\n",
        "val_df['original_with_context'] = val_df.apply(lambda x: make_context_sents(x.paragraph, x.pos), axis = 1)\n",
        "val_df['original_with_context'] = val_df['original_with_context'].apply(lambda x: (' '.join(x)).strip())\n",
        "\n",
        "\n",
        "def add_context_to_target(para, pos, target):\n",
        "    if pos + 1 < len(para) and pos - 1 >= 0:\n",
        "        return para[pos - 1] + target + para[pos + 1]\n",
        "    elif pos - 1 >= 0:\n",
        "        return para[pos - 1] + target\n",
        "    elif pos + 1 < len(para):\n",
        "        return target + para[pos + 1]\n",
        "    else:\n",
        "        return para[pos]\n",
        "\n",
        "train_df['corrected_with_context'] = train_df.apply(lambda x: add_context_to_target(x.paragraph, x.pos, x.corrected), axis = 1)\n",
        "#train_df['corrected_with_context'] = train_df['corrected_with_context'].apply(lambda x: (' '.join(x)).strip())\n",
        "\n",
        "test_df['corrected_with_context'] = test_df.apply(lambda x: add_context_to_target(x.paragraph, x.pos, x.corrected), axis = 1)\n",
        "#test_df['corrected_with_context'] = test_df['corrected_with_context'].apply(lambda x: (' '.join(x)).strip())\n",
        "\n",
        "val_df['corrected_with_context'] = val_df.apply(lambda x: add_context_to_target(x.paragraph, x.pos, x.corrected), axis = 1)\n",
        "#val_df['corrected_with_context'] = val_df['corrected_with_context'].apply(lambda x: (' '.join(x)).strip())\n",
        "\n",
        "print(train_df)"
      ],
      "metadata": {
        "id": "2-ZaXIK3klHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_train_val_df = pd.concat([train_df, val_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "2CMwCx9b5mTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding special tokens\n",
        "combined_train_val_df['corrected_with_context'] =combined_train_val_df.corrected.apply(lambda x: 'sos '+ x + ' eos')\n",
        "\n",
        "# Convert into list of sentence we need list to pass in tokenizer\n",
        "org_texts = combined_train_val_df.original_with_context.to_list()\n",
        "cor_texts = combined_train_val_df.corrected_with_context.to_list()\n",
        "\n",
        "def tokenize_sent(text):\n",
        "  '''\n",
        "  Take list on texts as input and\n",
        "  returns its tokenizer and enocoded text\n",
        "  '''\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(text)\n",
        "\n",
        "  return tokenizer, tokenizer.texts_to_sequences(text)\n",
        "\n",
        "\n",
        "# Tokenize\n",
        "org_tokenizer, org_encoded= tokenize_sent(text= org_texts)\n",
        "cor_tokenizer, cor_encoded= tokenize_sent(text= cor_texts)\n",
        "\n",
        "# Original Word --> index dictionary\n",
        "org_index_word = org_tokenizer.index_word\n",
        "\n",
        "# Orignal Index --> word dictionary\n",
        "org_word_index= org_tokenizer.word_index\n",
        "\n",
        "# size of Original vocabulary for encoder input\n",
        "# For zero padding we have to add +1 in size\n",
        "ORG_VOCAB_SIZE = len(org_tokenizer.word_counts)+1\n",
        "\n",
        "# Corrected Word --> index dict\n",
        "cor_word_index= cor_tokenizer.word_index\n",
        "\n",
        "# Corrected Index --> word dict\n",
        "cor_index_word = cor_tokenizer.index_word\n",
        "\n",
        "# Cor vocab size for decoder output\n",
        "COR_VOCAB_SIZE=len(cor_tokenizer.word_counts)+1\n",
        "\n",
        "# Getting max length of org and cor sentences\n",
        "max_org_len = 0\n",
        "for i in range(len(org_encoded)):\n",
        "  if len(org_encoded[i]) > max_org_len:\n",
        "    max_org_len= len(org_encoded[i])\n",
        "\n",
        "max_cor_len = 0\n",
        "for i in range(len(cor_encoded)):\n",
        "  if len(org_encoded[i]) > max_cor_len:\n",
        "    max_cor_len= len(cor_encoded[i])\n",
        "\n",
        "\n",
        "# Padding both\n",
        "org_padded = pad_sequences(org_encoded, maxlen=max_org_len, padding='post')\n",
        "cor_padded = pad_sequences(cor_encoded, maxlen=max_cor_len, padding='post')\n",
        "\n",
        "# Convert to array\n",
        "org_padded= np.array(org_padded)\n",
        "cor_padded= np.array(cor_padded)\n"
      ],
      "metadata": {
        "id": "23gOtqiNm7_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val = org_padded[:28066+1], org_padded[28066+1:]\n",
        "y_train, y_val = cor_padded[:28066+1], cor_padded[28066+1:]"
      ],
      "metadata": {
        "id": "cECCkVhKqOUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, Concatenate\n",
        "from tensorflow.keras import Input, Model\n",
        "\n",
        "# Encoder input\n",
        "encoder_inputs = Input(shape=(max_org_len,))\n",
        "\n",
        "# Embedding layer- i am using 1024 output-dim for embedding you can try diff values 100,256,512,1000\n",
        "enc_emb = Embedding(ORG_VOCAB_SIZE, 1024)(encoder_inputs)\n",
        "\n",
        "# Bidirectional lstm layer\n",
        "enc_lstm1 = Bidirectional(LSTM(256,return_sequences=True,return_state=True))\n",
        "encoder_outputs1, forw_state_h, forw_state_c, back_state_h, back_state_c = enc_lstm1(enc_emb)\n",
        "\n",
        "# Concatenate both h and c\n",
        "final_enc_h = Concatenate()([forw_state_h,back_state_h])\n",
        "final_enc_c = Concatenate()([forw_state_c,back_state_c])\n",
        "\n",
        "# get Context vector\n",
        "encoder_states =[final_enc_h, final_enc_c]"
      ],
      "metadata": {
        "id": "FG8NguSYqTjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"attention.ipynb\n",
        "Automatically generated by Colaboratory.\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1XrjPL3O_szhahYZW0z9yhCl9qvIcJJYW\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "metadata": {
        "id": "vK5TnFBnqXhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# decoder embedding with same number as encoder embedding\n",
        "dec_emb_layer = Embedding(COR_VOCAB_SIZE, 1024)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)   # apply this way because we need embedding layer for prediction\n",
        "\n",
        "# In encoder we used Bidirectional so it's having two LSTM's so we have to take double units(256*2=512) for single decoder lstm\n",
        "# LSTM using encoder's final states as initial state\n",
        "decoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "# Using Attention Layer\n",
        "attention_layer = AttentionLayer()\n",
        "attention_result, attention_weights = attention_layer([encoder_outputs1, decoder_outputs])\n",
        "\n",
        "# Concat attention output and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])\n",
        "\n",
        "# Dense layer with softmax\n",
        "decoder_dense = Dense(COR_VOCAB_SIZE, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "2OW0AtNOqqxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Define callbacks\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "checkpoint = ModelCheckpoint(\"MODEL_CHECKPOINTS\", monitor='val_accuracy')\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "callbacks_list = [checkpoint, early_stopping]\n",
        "\n",
        "# Training set\n",
        "encoder_input_data = X_train\n",
        "# To make same as target data skip last number which is just padding\n",
        "decoder_input_data = y_train[:,:-1]\n",
        "# Decoder target data has to be one step ahead so we are taking from 1 as told in keras docs\n",
        "decoder_target_data =  y_train[:,1:]\n",
        "\n",
        "# devlopment set\n",
        "encoder_input_test = X_val\n",
        "decoder_input_test = y_val[:,:-1]\n",
        "decoder_target_test=  y_val[:,1:]\n",
        "\n",
        "history = model.fit([encoder_input_data, decoder_input_data],decoder_target_data,\n",
        "                    epochs=5,\n",
        "                    batch_size=32,\n",
        "                    validation_data = ([encoder_input_test, decoder_input_test],decoder_target_test),\n",
        "                    callbacks= callbacks_list)\n",
        "\n",
        "# Don't forget to save weights of trained model\n",
        "model.save_weights(\"model_sent.h5\") # can give whole path to save model"
      ],
      "metadata": {
        "id": "KzBUTTELqu01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Its good to restart runtime and create model and load weights\n",
        "model.load_weights(\"model_sent.h5\")\n",
        "\n",
        "# INFERENCE MODEL\n",
        "# encoder Inference model\n",
        "encoder_model = Model(encoder_inputs, outputs = [encoder_outputs1, final_enc_h, final_enc_c])\n",
        "\n",
        "# Decoder Inference\n",
        "decoder_state_h = Input(shape=(512,)) # This numbers has to be same as units of lstm's on which model is trained\n",
        "decoder_state_c = Input(shape=(512,))\n",
        "\n",
        "# we need hidden state for attention layer\n",
        "decoder_hidden_state_input = Input(shape=(max_org_len,512))\n",
        "# get decoder states\n",
        "dec_states = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "# embedding layer\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states)\n",
        "\n",
        "# Attention inference\n",
        "attention_result_inf, attention_weights_inf = attention_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([decoder_outputs2, attention_result_inf])\n",
        "\n",
        "dec_states2= [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_concat_input_inf)\n",
        "\n",
        "# get decoder model\n",
        "decoder_model= Model(\n",
        "                    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_h, decoder_state_c],\n",
        "                     [decoder_outputs2]+ dec_states2)"
      ],
      "metadata": {
        "id": "nXtkNfX1wVk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicted_sentence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    enc_output, enc_h, enc_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = cor_word_index['sos']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [enc_output, enc_h, enc_c ])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        # convert max index number to marathi word\n",
        "        sampled_char = cor_index_word[sampled_token_index]\n",
        "        # aapend it to decoded sent\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length or find stop token.\n",
        "        if (sampled_char == 'eos' or len(decoded_sentence.split()) >= max_org_len):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        enc_h, enc_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "cqIyXmX1q0E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cor_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0 :\n",
        "        sentence =sentence +cor_index_word[i]+' '\n",
        "    return sentence\n",
        "\n",
        "def get_org_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0:\n",
        "        sentence =sentence +org_index_word[i]+' '\n",
        "    return sentence\n",
        "\n",
        "# using simple loop we will take 15 random numbers from x_test and get results\n",
        "for i in np.random.randint(10, 1000, size=15):\n",
        "  print(\"Org Sentence:\",get_org_sentence(X_val[i]))\n",
        "  print(\"Cor Sentence:\",get_cor_sentence(y_val[i])[4:-4])\n",
        "  # Before passing input it has to be reshape as following\n",
        "  print(\"Predicted Cor:\",get_predicted_sentence(X_val[i].reshape(1,max_org_len))[:-4])\n",
        "  print(\"----------------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "NlqJcToOq6W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# decoder embedding with same number as encoder embedding\n",
        "dec_emb_layer = Embedding(COR_VOCAB_SIZE, 1024)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)   # apply this way because we need embedding layer for prediction\n",
        "\n",
        "# In encoder we used Bidirectional so it's having two LSTM's so we have to take double units(256*2=512) for single decoder lstm\n",
        "# LSTM using encoder's final states as initial state\n",
        "decoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "# Using Attention Layer\n",
        "attention_layer = AttentionLayer()\n",
        "attention_result, attention_weights = attention_layer([encoder_outputs1, decoder_outputs])\n",
        "\n",
        "# Concat attention output and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])\n",
        "\n",
        "# Dense layer with softmax\n",
        "decoder_dense = Dense(COR_VOCAB_SIZE, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "InSer3-yxubz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint = ModelCheckpoint(\"MODEL_CHECKPOINTS\", monitor='val_accuracy')\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "callbacks_list = [checkpoint, early_stopping]\n",
        "\n",
        "# Training set\n",
        "encoder_input_data = X_train\n",
        "# To make same as target data skip last number which is just padding\n",
        "decoder_input_data = y_train[:,:-1]\n",
        "# Decoder target data has to be one step ahead so we are taking from 1 as told in keras docs\n",
        "decoder_target_data =  y_train[:,1:]\n",
        "\n",
        "# devlopment set\n",
        "encoder_input_test = X_val\n",
        "decoder_input_test = y_val[:,:-1]\n",
        "decoder_target_test=  y_val[:,1:]\n",
        "\n",
        "history = model.fit([encoder_input_data, decoder_input_data],decoder_target_data,\n",
        "                    epochs=1,\n",
        "                    batch_size=32,\n",
        "                    validation_data = ([encoder_input_test, decoder_input_test],decoder_target_test),\n",
        "                    callbacks= callbacks_list)\n",
        "\n",
        "# Don't forget to save weights of trained model\n",
        "model.save_weights(\"model_sent.h5\") # can give whole path to save model"
      ],
      "metadata": {
        "id": "HAvZUdHNxubz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Its good to restart runtime and create model and load weights\n",
        "model.load_weights(\"model_sent.h5\")\n",
        "\n",
        "# INFERENCE MODEL\n",
        "# encoder Inference model\n",
        "encoder_model = Model(encoder_inputs, outputs = [encoder_outputs1, final_enc_h, final_enc_c])\n",
        "\n",
        "# Decoder Inference\n",
        "decoder_state_h = Input(shape=(512,)) # This numbers has to be same as units of lstm's on which model is trained\n",
        "decoder_state_c = Input(shape=(512,))\n",
        "\n",
        "# we need hidden state for attention layer\n",
        "decoder_hidden_state_input = Input(shape=(max_org_len,512))\n",
        "# get decoder states\n",
        "dec_states = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "# embedding layer\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states)\n",
        "\n",
        "# Attention inference\n",
        "attention_result_inf, attention_weights_inf = attention_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([decoder_outputs2, attention_result_inf])\n",
        "\n",
        "dec_states2= [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_concat_input_inf)\n",
        "\n",
        "# get decoder model\n",
        "decoder_model= Model(\n",
        "                    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_h, decoder_state_c],\n",
        "                     [decoder_outputs2]+ dec_states2)"
      ],
      "metadata": {
        "id": "IEJ1qkqfxubz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicted_sentence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    enc_output, enc_h, enc_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = cor_word_index['sos']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [enc_output, enc_h, enc_c ])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        # convert max index number to marathi word\n",
        "        sampled_char = cor_index_word[sampled_token_index]\n",
        "        # aapend it to decoded sent\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length or find stop token.\n",
        "        if (sampled_char == 'eos' or len(decoded_sentence.split()) >= max_org_len):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        enc_h, enc_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "jf4zqFwYxubz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cor_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0 :\n",
        "        sentence =sentence +cor_index_word[i]+' '\n",
        "    return sentence\n",
        "\n",
        "def get_org_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0:\n",
        "        sentence =sentence +org_index_word[i]+' '\n",
        "    return sentence\n",
        "\n",
        "# using simple loop we will take 15 random numbers from x_test and get results\n",
        "for i in np.random.randint(10, 1000, size=15):\n",
        "  print(\"Org Sentence:\",get_org_sentence(X_val[i]))\n",
        "  print(\"Cor Sentence:\",get_cor_sentence(y_val[i])[4:-4])\n",
        "  # Before passing input it has to be reshape as following\n",
        "  print(\"Predicted Cor:\",get_predicted_sentence(X_val[i].reshape(1,max_org_len))[:-4])\n",
        "  print(\"----------------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "buoOlG6Rxubz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertviz"
      ],
      "metadata": {
        "id": "rroHy0Mky6En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import BartModel, BartForConditionalGeneration, BartTokenizerFast\n",
        "\n",
        "model = BartModel.from_pretrained(\"gotutiyan/gec-bart-base\", output_attentions=True)"
      ],
      "metadata": {
        "id": "VQ0AjNzfA75w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizerFast.from_pretrained(\"gotutiyan/gec-bart-base\")"
      ],
      "metadata": {
        "id": "JJzrlXgLBkO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CUSTOM_TESTS = [\n",
        "    ['She saw a cat.', 'He screams out loud.'],  # PRON, VERB:TENSE\n",
        "    ['The P versus NP problem is an unsolved problem in computer science.', 'No one has solved them to this day.'],  # PRON\n",
        "    ['The Millennium Prize Problems are seven very complex mathematical problems.', 'No one has solved it to this day.'],\n",
        "    ['Car crashes are easily preventable.', 'Most cases occurred because the driver was careless.'],  # VERB:TENSE\n",
        "    ['A study was done on 1000 car crashes.', 'Most cases occur because the driver is careless.'],\n",
        "    [\"If he thinks about it more, I'm sure he'll figure something out.\", 'The right idea eventually came to him.'],  # VERB:TENSE\n",
        "    ['The right idea will eventually come to him.', 'Many weeks of effort finally paid off.'],\n",
        "    ['Everyone knows that cats are adorable.', 'But they make for great companions.'],  # CONJ\n",
        "    ['Cats can be annoying at times.', 'And they make for great companions.'],\n",
        "    ['I visit the apple store frequently.', \"I'm always eager to check out the latest phone.\"],  # ORTH\n",
        "    ['I visit the apple store frequently.', 'Fruit works great as a snack.'],\n",
        "    ['Tom told his sister there was a spider in her hair.', 'Cried out in alarm.'],  # PRON\n",
        "    ['There have been complaints about long queues in the canteens.', \"I'm looking them now.\"],  # PREP\n",
        "    [\"I lost my earphones earlier.\", \"I'm looking them now.\"]\n",
        "]"
      ],
      "metadata": {
        "id": "rmAftehbBVfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install errant"
      ],
      "metadata": {
        "id": "0H1cWuAwyb9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!errant_compare -hyp \"/content/bart_sf3500_test.m2\" -ref  '/content/drive/MyDrive/datasets/wi+locness/dataset_splits/test.m2'"
      ],
      "metadata": {
        "id": "kpBKbNSlyeCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VISUALIZING ATTENTION"
      ],
      "metadata": {
        "id": "ysRYGhPRy4Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertviz"
      ],
      "metadata": {
        "id": "GFryNAWHCJtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import model_view"
      ],
      "metadata": {
        "id": "H_JVaTuTDnOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I eated dinner tonight because I am hungry\"\n",
        "encoder_input_ids = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).input_ids\n",
        "with tokenizer.as_target_tokenizer():\n",
        "  decoder_input_ids = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).input_ids\n",
        "  outputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n",
        "  encoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\n",
        "  decoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])"
      ],
      "metadata": {
        "id": "YN6A89WFA96s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import model_view\n",
        "model_view(\n",
        "    encoder_attention=outputs.encoder_attentions,\n",
        "    decoder_attention=outputs.decoder_attentions,\n",
        "    cross_attention=outputs.cross_attentions,\n",
        "    encoder_tokens= encoder_text,\n",
        "    decoder_tokens = decoder_text\n",
        ")"
      ],
      "metadata": {
        "id": "VlKF0SVZB6D3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}