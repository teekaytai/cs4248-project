{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch torchvision\n",
    "%pip install accelerate\n",
    "%pip install transformers\n",
    "%pip install transformers[torch]\n",
    "%pip install datasets\n",
    "%pip install errant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import spacy\n",
    "import errant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_PATH = '/content/drive/MyDrive/CS4248/dev.json'\n",
    "DEV_M2_PATH = 'dev.m2'\n",
    "OUT_M2_PATH = 'out.m2'\n",
    "\n",
    "MODEL_PATH = '/content/results_t5_small/checkpoint-3500'\n",
    "T5_MODEL = 't5-small'\n",
    "\n",
    "TASK_PREFIX = 'rectify'\n",
    "TOKENIZER_PADDING = 'max_length'\n",
    "SOURCE_MAX_LENGTH = 512\n",
    "GEN_MAX_LENGTH = 512\n",
    "GEN_NUM_BEAMS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = load_dataset('json', data_files=DEV_PATH, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to be tested\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "annotator = errant.load('en', nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correction(model, tokenizer, sample):\n",
    "    input_text = f\"{TASK_PREFIX}: {sample['original']}\"\n",
    "    inputs = t5_tokenizer.encode(\n",
    "        input_text,\n",
    "        max_length=SOURCE_MAX_LENGTH,\n",
    "        padding=TOKENIZER_PADDING,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    corrected_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=GEN_MAX_LENGTH,\n",
    "        num_beams=GEN_NUM_BEAMS,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    corrected_sentence = tokenizer.decode(\n",
    "        corrected_ids[0],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    # Retokenize sentence using spacy to restore correct spacing between tokens\n",
    "    # for accurate error correction score calculation\n",
    "    corrected_sentence = ' '.join(tok.text for tok in spacy_tokenizer(corrected_sentence))\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOOP_EDIT = 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'\n",
    "\n",
    "# Can use later for analysing performance for each type of error\n",
    "output_edit_types = []\n",
    "\n",
    "with open(OUT_M2_PATH, 'w') as f:\n",
    "    for sample in dataset_test:\n",
    "        orig = sample['original']\n",
    "        corrected = generate_correction(model, t5_tokenizer, sample)\n",
    "        edits = annotator.annotate(annotator.parse(orig), annotator.parse(corrected))\n",
    "        output_edit_types.append([edit.type for edit in edits])\n",
    "        print('S', orig, file=f)\n",
    "        if not edits:\n",
    "            print(NOOP_EDIT, file=f)\n",
    "        for edit in edits:\n",
    "            print(edit.to_m2(), file=f)\n",
    "        print(file=f)  # Blank divider line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare output edits with gold-standard edits and computes statistics\n",
    "!errant_compare -hyp {OUT_M2_PATH} -ref {DEV_M2_PATH}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
