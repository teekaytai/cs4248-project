# How Can I Explore This Branch?
Please go to the `[WI_LOCNESS]_Final_Experiments.ipynb` notebook or navigate to this Colab notebook: https://colab.research.google.com/drive/1gbe9GhIShbHhhSpDyYlgvcah5SbA0rWR?usp=sharing in order to view the experiments!

# What Can I Expect?
You may expect to see our trials of BiLSTM, made following this tutorial (https://medium.com/geekculture/neural-machine-translation-using-seq2seq-model-with-attention-9faea357d70b), as well as BART model experiments. The notebook has been sectioned to help with navigation!

> Note: There is an attempt of visualisation of attention to support analysis at the very end of the notebook. While I ran out of GPU units (despite a Colab Pro subscription and a purchase of extra units), if you have any suggestions on how visualisation of attention can be explored further to bring explainability to such projects, I would be glad to connect.
